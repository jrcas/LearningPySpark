{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Imports\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import expr, when, sum, avg, desc, asc\n",
    "# Also can from pyspark.sql import functions as F\n",
    "\n",
    "# ML Pyspark Imports\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Other Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Get heart data from Kaggle: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data all as strings\n",
    "df = spark.read.option('header', 'true').csv('/data/heart.csv')\n",
    "\n",
    "# Importing all the data with column types, pass it as an argument in the function\n",
    "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "df = spark.read.csv('/data/heart.csv', schema= schema, header=True)\n",
    "\n",
    "# Infer the schema based on the data in the column\n",
    "df = spark.read.csv('/data/heart.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Replace null values with any other value\n",
    "df = spark.read.csv('/data/heart.csv', inferSchema=True, nullValue='NA', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non overwrittable\n",
    "df.write.format(\"csv\").save(\"/heart_save.csv\")\n",
    "\n",
    "# Overwrittable\n",
    "df.write.format(\"csv\").save(\"/heart_save.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of rows\n",
    "df.count()\n",
    "\n",
    "# Showing the first x rows\n",
    "df.show(5)\n",
    "\n",
    "# Showing a specific column\n",
    "df.select('Age').show(5)\n",
    "df.select(['Age', 'Sex']).show(5)\n",
    "\n",
    "# PySpark to Pandas\n",
    "pd_df = df.toPandas()\n",
    "\n",
    "# Pandas to PySpark\n",
    "spark_df = spark.createDataFrame(pd_df)\n",
    "\n",
    "# To see column data type and see if they accept null values\n",
    "df.printSchema()\n",
    "# or\n",
    "df.dtypes\n",
    "\n",
    "# Change column data type to float for example\n",
    "df = df.withcolumn(\"Age\", df.Age.cast(FloatType()))\n",
    "\n",
    "# To remove a column\n",
    "df.drop(\"AgeFixed\")\n",
    "\n",
    "# To Rename a column\n",
    "df.withColumnRenamed('Age','age')\n",
    "# or\n",
    "name_columns = {'Age':'age', 'Sex': 'sex'}\n",
    "df.withColumnsRenamed(name_columns)\n",
    "\n",
    "# Table of Statistics\n",
    "df.select(['Age','RestingBP']).describe().show()\n",
    "\n",
    "# Managing NA\n",
    "df = df.na.drop() # Drop all rows that contain a single null value\n",
    "df = df.na.drop(how='all') # Drop rows that have nothing but null values\n",
    "df = df.na.drop(thresh=2) # Drop rows that have 2 or more null values\n",
    "df = df.na.drop(how = 'any', subset = ['age','sex']) # Drop rows that have NA based on columns\n",
    "df = df.na.fill(value='?', subset=['sex']) # Replace null values with a different value\n",
    "\n",
    "imptr = Imputer(inputCols=['age','RestingBP'], outputCols=['age','RestingBP']).setStrategy('mean') # Create an imputer to set the missing values\n",
    "df = imptr.fit(df).transform(df) # Fit the changes to the dataframe to fill in the missing values\n",
    "\n",
    "# Filtering\n",
    "df.filter('age > 18')\n",
    "df.where('age > 18')\n",
    "df.where(df['age'] > 18)\n",
    "df.where((df['age'] > 18) & (df['ChestPainType'] == 'ATA')) # AND operator\n",
    "df.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA')) # OR operator\n",
    "df.filter(~(df['ChestPainType'] == 'ATA')) # Choose all rows that do not match the behavior\n",
    "\n",
    "# Evaluating a string\n",
    "exp = 'age + 0.2 * AgeFixed'\n",
    "df.withColumn('new_col', expr(exp))\n",
    "\n",
    "# Multiplying columns\n",
    "df.withColumn('new_col', df.age * 0.2)\n",
    "df.withColumn('new_col', when(df.ChestPainType == 'ATA', 0).otherwise(df.age * 0.2))\n",
    "\n",
    "# Group by\n",
    "df.groupby('age').agg(avg('HeartDisease').alias('avg_HeartDisease')).show()\n",
    "\n",
    "# Ordering by asc or descending\n",
    "disease_by_age = df.groupby('Age').agg(avg('HeartDisease').alias('avg_HeartDisease')) # Can continue adding new and different aggregations\n",
    "disease_by_age.orderBy(asc('Age')).show()\n",
    "\n",
    "# Pivoting\n",
    "df.groupby('age').pivot('sex', (\"M\", \"F\")).count() # Is computational expensive but is lessened by providing the values you want to pivot on to the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using SQL Syntax\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"SELECT Sex from df\"\"\").show(2)\n",
    "\n",
    "# Using a regular Select Expresion\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\", \"HeartDisease\").show(2) # Can continue adding more columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Comands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|sex|true|false|\n",
      "+---+----+-----+\n",
      "|  F| 174|   19|\n",
      "|  M| 664|   61|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just an example\n",
    "df.selectExpr(\"Age >= 40 as older\", \"Age\", \"Sex\").groupBy(\"sex\")\\\n",
    "    .pivot(\"older\", (\"true\", \"false\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9938204489815767,0.04816126113055623]\n",
      "180.57905208099078\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+------------+------------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|        Fvec|        prediction|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+------------+------------------+\n",
      "| 31|  M|          ASY|      120|        270|        0|    Normal|  153|             Y|    1.5|    Flat|           1|[31.0,270.0]| 162.7741586678121|\n",
      "| 34|  M|           TA|      140|        156|        0|    Normal|  180|             N|    0.0|    Flat|           1|[34.0,156.0]|154.30231355198393|\n",
      "| 35|  F|          ASY|      140|        167|        0|    Normal|  150|             N|    0.0|      Up|           0|[35.0,167.0]| 153.8382669754385|\n",
      "| 35|  F|           TA|      120|        160|        0|        ST|  185|             N|    0.0|      Up|           0|[35.0,160.0]|153.50113814752459|\n",
      "| 37|  F|          NAP|      130|        211|        0|    Normal|  142|             N|    0.0|      Up|           0|[37.0,211.0]| 153.9697215672198|\n",
      "| 37|  M|          NAP|      118|        240|        0|       LVH|  165|             N|    1.0|    Flat|           0|[37.0,240.0]|155.36639814000594|\n",
      "| 38|  M|          ASY|      110|        196|        0|    Normal|  166|             N|    0.0|    Flat|           1|[38.0,196.0]| 152.2534822012799|\n",
      "| 38|  M|          ASY|      135|          0|        1|    Normal|  150|             N|    0.0|    Flat|           1|  [38.0,0.0]|142.81387501969087|\n",
      "| 38|  M|          ASY|      150|          0|        1|    Normal|  120|             Y|    0.7|    Flat|           1|  [38.0,0.0]|142.81387501969087|\n",
      "| 38|  M|          NAP|      100|          0|        0|    Normal|  179|             N|   -1.1|      Up|           0|  [38.0,0.0]|142.81387501969087|\n",
      "| 39|  M|          ASY|      110|        280|        0|    Normal|  150|             N|    0.0|    Flat|           1|[39.0,280.0]|155.30520768726504|\n",
      "| 39|  M|          ATA|      120|        200|        0|    Normal|  160|             Y|    1.0|    Flat|           0|[39.0,200.0]|151.45230679682055|\n",
      "| 40|  M|          ASY|      110|        167|        0|       LVH|  114|             Y|    2.0|    Flat|           1|[40.0,167.0]| 148.8691647305306|\n",
      "| 41|  M|          ASY|      104|          0|        0|        ST|  111|             N|    0.0|      Up|           0|  [41.0,0.0]|139.83241367274613|\n",
      "| 41|  M|          ASY|      130|        172|        0|        ST|  130|             N|    2.0|    Flat|           1|[41.0,172.0]| 148.1161505872018|\n",
      "| 41|  M|          ASY|      150|        171|        0|    Normal|  128|             Y|    1.5|    Flat|           0|[41.0,171.0]|148.06798932607126|\n",
      "| 41|  M|          ATA|      125|        269|        0|    Normal|  144|             N|    0.0|      Up|           0|[41.0,269.0]|152.78779291686575|\n",
      "| 41|  M|          ATA|      135|        203|        0|    Normal|  132|             N|    0.0|    Flat|           0|[41.0,203.0]|149.60914968224904|\n",
      "| 41|  M|          NAP|      112|        250|        0|    Normal|  179|             N|    0.0|      Up|           0|[41.0,250.0]|151.87272895538518|\n",
      "| 41|  M|          NAP|      130|        214|        0|       LVH|  168|             N|    2.0|    Flat|           0|[41.0,214.0]|150.13892355468516|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark ML does not accept a dataframe but instead you need to make it into one column that has a vector per row with each of the values\n",
    "feature_col_names = ['Age', 'Cholesterol']\n",
    "v_asemblr = VectorAssembler(inputCols=feature_col_names, outputCol='Fvec')\n",
    "model_df = v_asemblr.transform(df)\n",
    "\n",
    "# Using a model\n",
    "trainset, testset = model_df.randomSplit([0.8, 0.2]) # Division proportion \n",
    "model = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\n",
    "model = model.fit(trainset)\n",
    "\n",
    "print(model.coefficients)\n",
    "print(model.intercept)\n",
    "\n",
    "model.evaluate(testset).predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
