{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Imports\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import expr, when, sum, avg, desc, asc\n",
    "# Also can from pyspark.sql import functions as F\n",
    "\n",
    "# ML Pyspark Imports\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Other Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYSPARK_DRIVER_PYTHON\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Creating a Spark Session\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\BigDataProject\\Lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\BigDataProject\\Lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\BigDataProject\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\BigDataProject\\Lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\BigDataProject\\Lib\\site-packages\\pyspark\\java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 103\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# System paths\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Creating a Spark Session\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Get heart data from Kaggle: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data all as strings\n",
    "df = spark.read.option('header', 'true').csv('heart.csv')\n",
    "\n",
    "# Importing all the data with column types, pass it as an argument in the function\n",
    "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "df = spark.read.csv('heart.csv', schema= schema, header=True)\n",
    "\n",
    "# Infer the schema based on the data in the column\n",
    "df = spark.read.csv('heart.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Replace null values with any other value\n",
    "df = spark.read.csv('heart.csv', inferSchema=True, nullValue='NA', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non overwrittable\n",
    "df.write.format(\"csv\").save(\"/heart_save.csv\")\n",
    "\n",
    "# Overwrittable\n",
    "df.write.format(\"csv\").save(\"/heart_save.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of rows\n",
    "df.count()\n",
    "\n",
    "# Showing the first x rows\n",
    "df.show(5)\n",
    "\n",
    "# Showing a specific column\n",
    "df.select('Age').show(5)\n",
    "df.select(['Age', 'Sex']).show(5)\n",
    "\n",
    "# PySpark to Pandas\n",
    "pd_df = df.toPandas()\n",
    "\n",
    "# Pandas to PySpark\n",
    "spark_df = spark.createDataFrame(pd_df)\n",
    "\n",
    "# To see column data type and see if they accept null values\n",
    "df.printSchema()\n",
    "# or\n",
    "df.dtypes\n",
    "\n",
    "# Change column data type to float for example\n",
    "df = df.withcolumn(\"Age\", df.Age.cast(FloatType()))\n",
    "\n",
    "# To remove a column\n",
    "df.drop(\"AgeFixed\")\n",
    "\n",
    "# To Rename a column\n",
    "df.withColumnRenamed('Age','age')\n",
    "# or\n",
    "name_columns = {'Age':'age', 'Sex': 'sex'}\n",
    "df.withColumnsRenamed(name_columns)\n",
    "\n",
    "# Table of Statistics\n",
    "df.select(['Age','RestingBP']).describe().show()\n",
    "\n",
    "# Managing NA\n",
    "df = df.na.drop() # Drop all rows that contain a single null value\n",
    "df = df.na.drop(how='all') # Drop rows that have nothing but null values\n",
    "df = df.na.drop(thresh=2) # Drop rows that have 2 or more null values\n",
    "df = df.na.drop(how = 'any', subset = ['age','sex']) # Drop rows that have NA based on columns\n",
    "df = df.na.fill(value='?', subset=['sex']) # Replace null values with a different value\n",
    "\n",
    "imptr = Imputer(inputCols=['age','RestingBP'], outputCols=['age','RestingBP']).setStrategy('mean') # Create an imputer to set the missing values\n",
    "df = imptr.fit(df).transform(df) # Fit the changes to the dataframe to fill in the missing values\n",
    "\n",
    "# Filtering\n",
    "df.filter('age > 18')\n",
    "df.where('age > 18')\n",
    "df.where(df['age'] > 18)\n",
    "df.where((df['age'] > 18) & (df['ChestPainType'] == 'ATA')) # AND operator\n",
    "df.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA')) # OR operator\n",
    "df.filter(~(df['ChestPainType'] == 'ATA')) # Choose all rows that do not match the behavior\n",
    "\n",
    "# Evaluating a string\n",
    "exp = 'age + 0.2 * AgeFixed'\n",
    "df.withColumn('new_col', expr(exp))\n",
    "\n",
    "# Multiplying columns\n",
    "df.withColumn('new_col', df.age * 0.2)\n",
    "df.withColumn('new_col', when(df.ChestPainType == 'ATA', 0).otherwise(df.age * 0.2))\n",
    "\n",
    "# Group by\n",
    "df.groupby('age').agg(avg('HeartDisease').alias('avg_HeartDisease')).show()\n",
    "\n",
    "# Ordering by asc or descending\n",
    "disease_by_age = df.groupby('Age').agg(avg('HeartDisease').alias('avg_HeartDisease')) # Can continue adding new and different aggregations\n",
    "disease_by_age.orderBy(asc('Age')).show()\n",
    "\n",
    "# Pivoting\n",
    "df.groupby('age').pivot('sex', (\"M\", \"F\")).count() # Is computational expensive but is lessened by providing the values you want to pivot on to the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using SQL Syntax\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"SELECT Sex from df\"\"\").show(2)\n",
    "\n",
    "# Using a regular Select Expresion\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\", \"HeartDisease\").show(2) # Can continue adding more columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Comands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|sex|true|false|\n",
      "+---+----+-----+\n",
      "|  F| 174|   19|\n",
      "|  M| 664|   61|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just an example\n",
    "df.selectExpr(\"Age >= 40 as older\", \"Age\", \"Sex\").groupBy(\"sex\")\\\n",
    "    .pivot(\"older\", (\"true\", \"false\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9938204489815767,0.04816126113055623]\n",
      "180.57905208099078\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+------------+------------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|        Fvec|        prediction|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+------------+------------------+\n",
      "| 31|  M|          ASY|      120|        270|        0|    Normal|  153|             Y|    1.5|    Flat|           1|[31.0,270.0]| 162.7741586678121|\n",
      "| 34|  M|           TA|      140|        156|        0|    Normal|  180|             N|    0.0|    Flat|           1|[34.0,156.0]|154.30231355198393|\n",
      "| 35|  F|          ASY|      140|        167|        0|    Normal|  150|             N|    0.0|      Up|           0|[35.0,167.0]| 153.8382669754385|\n",
      "| 35|  F|           TA|      120|        160|        0|        ST|  185|             N|    0.0|      Up|           0|[35.0,160.0]|153.50113814752459|\n",
      "| 37|  F|          NAP|      130|        211|        0|    Normal|  142|             N|    0.0|      Up|           0|[37.0,211.0]| 153.9697215672198|\n",
      "| 37|  M|          NAP|      118|        240|        0|       LVH|  165|             N|    1.0|    Flat|           0|[37.0,240.0]|155.36639814000594|\n",
      "| 38|  M|          ASY|      110|        196|        0|    Normal|  166|             N|    0.0|    Flat|           1|[38.0,196.0]| 152.2534822012799|\n",
      "| 38|  M|          ASY|      135|          0|        1|    Normal|  150|             N|    0.0|    Flat|           1|  [38.0,0.0]|142.81387501969087|\n",
      "| 38|  M|          ASY|      150|          0|        1|    Normal|  120|             Y|    0.7|    Flat|           1|  [38.0,0.0]|142.81387501969087|\n",
      "| 38|  M|          NAP|      100|          0|        0|    Normal|  179|             N|   -1.1|      Up|           0|  [38.0,0.0]|142.81387501969087|\n",
      "| 39|  M|          ASY|      110|        280|        0|    Normal|  150|             N|    0.0|    Flat|           1|[39.0,280.0]|155.30520768726504|\n",
      "| 39|  M|          ATA|      120|        200|        0|    Normal|  160|             Y|    1.0|    Flat|           0|[39.0,200.0]|151.45230679682055|\n",
      "| 40|  M|          ASY|      110|        167|        0|       LVH|  114|             Y|    2.0|    Flat|           1|[40.0,167.0]| 148.8691647305306|\n",
      "| 41|  M|          ASY|      104|          0|        0|        ST|  111|             N|    0.0|      Up|           0|  [41.0,0.0]|139.83241367274613|\n",
      "| 41|  M|          ASY|      130|        172|        0|        ST|  130|             N|    2.0|    Flat|           1|[41.0,172.0]| 148.1161505872018|\n",
      "| 41|  M|          ASY|      150|        171|        0|    Normal|  128|             Y|    1.5|    Flat|           0|[41.0,171.0]|148.06798932607126|\n",
      "| 41|  M|          ATA|      125|        269|        0|    Normal|  144|             N|    0.0|      Up|           0|[41.0,269.0]|152.78779291686575|\n",
      "| 41|  M|          ATA|      135|        203|        0|    Normal|  132|             N|    0.0|    Flat|           0|[41.0,203.0]|149.60914968224904|\n",
      "| 41|  M|          NAP|      112|        250|        0|    Normal|  179|             N|    0.0|      Up|           0|[41.0,250.0]|151.87272895538518|\n",
      "| 41|  M|          NAP|      130|        214|        0|       LVH|  168|             N|    2.0|    Flat|           0|[41.0,214.0]|150.13892355468516|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark ML does not accept a dataframe but instead you need to make it into one column that has a vector per row with each of the values\n",
    "feature_col_names = ['Age', 'Cholesterol']\n",
    "v_asemblr = VectorAssembler(inputCols=feature_col_names, outputCol='Fvec')\n",
    "model_df = v_asemblr.transform(df)\n",
    "\n",
    "# Using a model\n",
    "trainset, testset = model_df.randomSplit([0.8, 0.2]) # Division proportion \n",
    "model = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\n",
    "model = model.fit(trainset)\n",
    "\n",
    "print(model.coefficients)\n",
    "print(model.intercept)\n",
    "\n",
    "model.evaluate(testset).predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
